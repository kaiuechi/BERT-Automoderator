{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data/reddit_200k_train.csv', encoding='ISO-8859-1') \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body.duplicated(keep=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.loc[df.body.duplicated(keep=False)].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['parent_id.x'].duplicated(keep=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['id'].duplicated(keep=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.REMOVED.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth', 400)\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.drop(['Unnamed: 0', 'score.x', 'parent_id.x', 'id', 'created_utc.x', 'retrieved_on'], axis=1)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.body = df_clean.body.str.lower()\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "df_clean['tokenized'] = df_clean['body'].apply(tokenizer.tokenize)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_top_10(freq_dist, title):\n",
    "\n",
    "    \n",
    "    top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "    tokens = top_10[0]\n",
    "    counts = top_10[1]\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(tokens, counts)\n",
    "\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    #ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    \n",
    "\n",
    "\n",
    "visualize_top_10(FreqDist(df_clean['tokenized'].explode()), 'word freq, train set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input(input_df):\n",
    "    df = input_df.copy() \n",
    "\n",
    "    \n",
    "    df.drop(['Unnamed: 0', 'score.x', 'parent_id.x', 'id', 'created_utc.x', 'retrieved_on'], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    df.body = df.body.str.lower()\n",
    "    \n",
    "    \n",
    "    pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    df['tokenized'] = df['body'].apply(tokenizer.tokenize)\n",
    "    \n",
    "    \n",
    "    df.drop(\n",
    "        df.loc[df['body'].str.contains('your submission has been removed for the following')].index,\n",
    "        inplace=True\n",
    "    )\n",
    "    return df\n",
    "\n",
    "train = clean_input(pd.read_csv('data/reddit_200k_train.csv', encoding='ISO-8859-1'))\n",
    "test = clean_input(pd.read_csv('data/reddit_200k_test.csv', encoding='ISO-8859-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.loc[df['body'].str.contains('your submission has been removed for the following')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train = train['body']\n",
    "y_train = train['REMOVED']\n",
    "X_test = test['body']\n",
    "y_test = test['REMOVED']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10)\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "baseline_model = MultinomialNB()\n",
    "\n",
    "\n",
    "baseline_cv = cross_val_score(baseline_model, X_train_vectorized, y_train)\n",
    "baseline_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(token_list): \n",
    "\n",
    "    result = []\n",
    "    for token in token_list:\n",
    "        if token not in stopwords_list:\n",
    "            result.append(token)\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "x_wo_stop = df_clean['tokenized'].apply(remove_stopwords)\n",
    "\n",
    "visualize_top_10(FreqDist(x_wo_stop.explode()), 'top 10 w/o stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10,\n",
    "    stop_words=stopwords_list\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train)\n",
    "\n",
    "base_remove_stop = MultinomialNB()\n",
    "\n",
    "remove_stop_cv = cross_val_score(base_remove_stop, X_train_vectorized, y_train)\n",
    "remove_stop_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(baseline_cv.mean())\n",
    "print(remove_stop_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "def stem_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "stemmed_stopwords = [stemmer.stem(word) for word in stopwords_list]\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10, \n",
    "    stop_words=stemmed_stopwords, \n",
    "    tokenizer=stem_and_tokenize)\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_stem = MultinomialNB()\n",
    "\n",
    "stemmed_cv = cross_val_score(base_stem, X_train_vectorized, y_train)\n",
    "stemmed_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(baseline_cv.mean())\n",
    "print(remove_stop_cv.mean())\n",
    "print(stemmed_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "train['sent_token'] = train['body'].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sent = train.drop('REMOVED', axis=1)\n",
    "X_train_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=500,\n",
    "    stop_words=stemmed_stopwords,\n",
    "    tokenizer=stem_and_tokenize\n",
    ")\n",
    "\n",
    "X_train_vectorized = tfidf.fit_transform(X_train_sent[\"body\"])\n",
    "\n",
    "X_train_vectorized_df = pd.DataFrame(X_train_vectorized.toarray(), columns=tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized_df['sent_token'] = train['sent_token']\n",
    "X_train_vectorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized_df.loc[X_train_vectorized_df['sent_token'].isna(), 'sent_token'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_NB = MultinomialNB()\n",
    "\n",
    "final_NB.fit(X_train_vectorized_df, y_train)\n",
    "final_NB.score(X_train_vectorized_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train.loc[train['body'].str.contains('has been removed because')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 400)\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(input_list):\n",
    "    output_list = []\n",
    "    for word in input_list:\n",
    "        if word not in stopwords_list:\n",
    "            output_list.append(word)\n",
    "    return output_list\n",
    "\n",
    "def clean_input(input_df):\n",
    "    df = input_df.copy() \n",
    "\n",
    "    \n",
    "    df.drop(['Unnamed: 0', 'score.x', 'parent_id.x', 'id', 'created_utc.x', 'retrieved_on'], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    df.body = df.body.str.lower()\n",
    "    \n",
    "    \n",
    "    df.drop(\n",
    "        df.loc[df['body'].str.contains('your submission has been removed for the following')].index,\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    df.drop(\n",
    "        df.loc[df['body'].str.contains('has been removed because')].index,\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    df['tokenized'] = df['body'].apply(tokenizer.tokenize)\n",
    "    \n",
    "    \n",
    "    df['no_stop'] = df['tokenized'].apply(remove_stopwords)\n",
    "    \n",
    "    \n",
    "    lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    df['lemma'] = df['no_stop'].apply(lambda x: [lemma.lemmatize(y) for y in x])\n",
    "    \n",
    "    \n",
    "    df['clean_string'] = df['lemma'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "train = clean_input(pd.read_csv('data/reddit_200k_train.csv', encoding='ISO-8859-1'))\n",
    "test = clean_input(pd.read_csv('data/reddit_200k_test.csv', encoding='ISO-8859-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_csv('data/train_clean.csv')\n",
    "#test.to_csv('data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w2v_train = train['clean_string'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "w2v_train_list = []\n",
    "for text in w2v_train:\n",
    "    list_words = text.split()\n",
    "    list_grams = [\" \".join(list_words[i:i+1]) \n",
    "                 for i in range(0, len(list_words), 1)]\n",
    "    w2v_train_list.append(list_grams)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#w2v_train_bi = []\n",
    "#for sent in w2v_train_list:\n",
    "#    sent_bi = list(zip(sent, sent[1:]))\n",
    "#    #print(sent_bi)\n",
    "#    output_sent = []\n",
    "#    for gram in sent_bi:\n",
    "#        output_gram = ' '.join(gram)\n",
    "#        output_sent.append(output_gram)\n",
    "#    w2v_train_bi.append(output_sent)\n",
    "    \n",
    "#check_idx = 300\n",
    "\n",
    "#print(w2v_train_list[check_idx])\n",
    "#print(w2v_train_bi[check_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(w2v_train_list))\n",
    "#print(len(w2v_train_bi))\n",
    "\n",
    "\n",
    "#w2v_train_final = w2v_train_list.copy()\n",
    "#w2v_train_final.extend(w2v_train_bi)\n",
    "#print(len(w2v_train_list))\n",
    "#print(len(w2v_train_bi))\n",
    "#print(len(w2v_train_final))\n",
    "\n",
    "#print(w2v_train_final[-8492])\n",
    "#print(w2v_train_bi[-8492])\n",
    "print(w2v_train_list[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#x = w2v_train_list[145]\n",
    "#print(x)\n",
    "#y = list(zip(x, x[1:], x[2:]))\n",
    "#print(y)\n",
    "#output_sent = []\n",
    "#for gram in y:\n",
    "#    output = ' '.join(gram)\n",
    "#    print(output)\n",
    "#    output_sent.append(output)\n",
    "#print(output_sent)\n",
    "\n",
    "#print('before', w2v_train_list[1])\n",
    "\n",
    "\n",
    "#get_bigrams = gensim.models.phrases.Phrases(w2v_train_list, \n",
    "#                                            delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "#bigrams_detector = gensim.models.phrases.Phraser(get_bigrams)\n",
    "\n",
    "#get_trigrams = gensim.models.phrases.Phrases(bigrams_detector[w2v_train_list], \n",
    "#                                                  delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "#trigrams_detector = gensim.models.phrases.Phraser(get_trigrams)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print('after', w2v_train_list[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#nlp = gensim.models.word2vec.Word2Vec(w2v_train_list, size=300,   \n",
    "#            window=8, min_count=1, sg=1, iter=30)\n",
    "\n",
    "#nlp.save('data/w2v2')\n",
    "\n",
    "#nlp = gensim.models.word2vec.Word2Vec.load('data/w2v1') \n",
    "nlp = gensim.models.word2vec.Word2Vec.load('data/w2v2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"data\"\n",
    "nlp[word].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow.keras.preprocessing\n",
    "\n",
    "tokenizer = tensorflow.keras.preprocessing.text.Tokenizer(lower=True, split=' ', \n",
    "                                                          oov_token=\"NaN\", \n",
    "                                                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(w2v_train_list)\n",
    "dic_vocabulary = tokenizer.word_index\n",
    "\n",
    "\n",
    "sequence_list= tokenizer.texts_to_sequences(w2v_train_list)\n",
    "\n",
    "\n",
    "X_train = tensorflow.keras.preprocessing.sequence.pad_sequences(sequence_list, \n",
    "                    maxlen=15, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w2v_test = test['clean_string'].copy()\n",
    "\n",
    "w2v_test_list = []\n",
    "for text in w2v_test:\n",
    "    list_words = text.split()\n",
    "    list_grams = [\" \".join(list_words[i:i+1])\n",
    "                 for i in range(0, len(list_words), 1)]\n",
    "    w2v_test_list.append(list_grams)\n",
    "    \n",
    "#w2v_test_list = list(bigrams_detector[w2v_test_list])\n",
    "#w2v_test_list = list(trigrams_detector[w2v_test_list])\n",
    "\n",
    "sequence_list = tokenizer.texts_to_sequences(w2v_test_list)\n",
    "X_test = tensorflow.keras.preprocessing.sequence.pad_sequences(sequence_list, maxlen=15,\n",
    "             padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_test==0, vmin=0, vmax=1, cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 0\n",
    "\n",
    "\n",
    "len_txt = len(train[\"clean_string\"].iloc[i].split())\n",
    "print(\"from: \", train[\"clean_string\"].iloc[i], \"| len:\", len_txt)\n",
    "\n",
    "\n",
    "len_tokens = len(X_train[i])\n",
    "print(\"to: \", X_train[i], \"| len:\", len(X_train[i]))\n",
    "\n",
    "\n",
    "print(\"check: \", train[\"clean_string\"].iloc[i].split()[0], \n",
    "      \" -- idx in vocabulary -->\", \n",
    "      dic_vocabulary[train[\"clean_string\"].iloc[i].split()[0]])\n",
    "\n",
    "print(\"vocabulary: \", dict(list(dic_vocabulary.items())[0:5]), \"... (padding element, 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
    "\n",
    "for word,idx in dic_vocabulary.items():\n",
    "    \n",
    "    try:\n",
    "        embeddings[idx] =  nlp[word]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word = \"active\"\n",
    "print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\n",
    "print(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape, \n",
    "      \"|vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "\n",
    "def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "x_in = layers.Input(shape=(15,))\n",
    "\n",
    "\n",
    "x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                     output_dim=embeddings.shape[1], \n",
    "                     weights=[embeddings],\n",
    "                     input_length=15, trainable=False)(x_in)\n",
    "\n",
    "\n",
    "#x = attention_layer(x, neurons=15)\n",
    "\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n",
    "                         return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2))(x)\n",
    "\n",
    "\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "y_out = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = models.Model(x_in, y_out)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = train['REMOVED'].values\n",
    "y_test = test['REMOVED'].values\n",
    "dic_y_mapping = {n:label for n,label in \n",
    "                 enumerate(np.unique(y_train))}\n",
    "inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "y_train = np.array([inverse_dic[y] for y in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#y_train = train['REMOVED'].values\n",
    "#y_test = test['REMOVED'].values\n",
    "\n",
    "#dic_y_mapping = {n:label for n,label in \n",
    "#                 enumerate(np.unique(y_train))}\n",
    "#inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "#y_train = np.array([inverse_dic[y] for y in y_train])\n",
    "\n",
    "\n",
    "training = model.fit(x=X_train, y=y_train, batch_size=256, \n",
    "                     epochs=10, shuffle=True, verbose=1, \n",
    "                     validation_split=0.3)\n",
    "\n",
    "\n",
    "model.save('data/lstm2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "ax[0].set(title=\"Training\")\n",
    "ax11 = ax[0].twinx()\n",
    "ax[0].plot(training.history['loss'], color='black')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "    ax11.plot(training.history[metric], label=metric)\n",
    "ax11.set_ylabel(\"Score\", color='steelblue')\n",
    "ax11.legend()\n",
    "ax[1].set(title=\"Validation\")\n",
    "ax22 = ax[1].twinx()\n",
    "ax[1].plot(training.history['val_loss'], color='black')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "     ax22.plot(training.history['val_'+metric], label=metric)\n",
    "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('data/lstm2')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "explainer = lime_text.LimeTextExplainer(verbose=True)\n",
    "explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([X_test.tolist()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select_idx = 200\n",
    "print(model.predict(np.array([X_test.tolist()[11]])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explanation = explainer.explain_instance(np.array([X_test.tolist()[11]]), classifier_fn=model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 400)\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "from lime import lime_text\n",
    "import tensorflow.keras.preprocessing\n",
    "import seaborn as sns\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_clean.csv')\n",
    "test = pd.read_csv('data/test_clean.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train.drop(train.loc[train['clean_string'].isna()].index, inplace=True)\n",
    "print(train['clean_string'].isna().sum())\n",
    "\n",
    "test.drop(test.loc[test['clean_string'].isna()].index, inplace=True)\n",
    "print(test['clean_string'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['REMOVED'].astype(int).to_numpy()\n",
    "y_test = test['REMOVED'].astype(int).to_numpy()\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_list = train['clean_string'].to_list()\n",
    "#train_list[50:100]\n",
    "train_list_tokenized = []\n",
    "for string in train_list:\n",
    "    #print(string)\n",
    "    train_list_tokenized.append(string.split(' '))\n",
    "    \n",
    "#print(type(train_list_tokenized[1]))\n",
    "print(train_list_tokenized[0:10])\n",
    "#print(len(train_list_tokenized))\n",
    "\n",
    "\n",
    "test_list = test['clean_string'].to_list()\n",
    "test_list_tokenized = []\n",
    "for string in test_list:\n",
    "    test_list_tokenized.append(string.split(' '))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "tokenizer = tensorflow.keras.preprocessing.text.Tokenizer(lower=False,\n",
    "                                                          split=' ', \n",
    "                                                          oov_token=\"NaN\", \n",
    "                                                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(train_list_tokenized)\n",
    "vocabulary = tokenizer.word_index\n",
    "#print(vocabulary)\n",
    "\n",
    "\n",
    "\n",
    "train_sequence = tokenizer.texts_to_sequences(train_list_tokenized)\n",
    "\n",
    "test_sequence = tokenizer.texts_to_sequences(test_list_tokenized)\n",
    "\n",
    "\n",
    "\n",
    "train_seq_padded = tensorflow.keras.preprocessing.sequence.pad_sequences(train_sequence, \n",
    "                                                                         maxlen=15, \n",
    "                                                                         padding=\"post\", \n",
    "                                                                         truncating=\"post\")\n",
    "\n",
    "test_seq_padded = tensorflow.keras.preprocessing.sequence.pad_sequences(test_sequence,\n",
    "                                                                        maxlen=15, \n",
    "                                                                        padding='post', \n",
    "                                                                        truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "to_check = train_seq_padded\n",
    "#to_check = test_seq_padded\n",
    "\n",
    "sns.heatmap(to_check==0, vmin=0, vmax=1, cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "w2v = gensim.models.word2vec.Word2Vec.load('data/w2v2') \n",
    "\n",
    "\n",
    "embed_matrix = np.zeros((len(vocabulary)+1, 300))\n",
    "\n",
    "for word, index in vocabulary.items():\n",
    "    try:\n",
    "        embed_matrix[index] =  w2v[word]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "word = \"movie\"\n",
    "print(\"word index in vocab:\", vocabulary[word])\n",
    "print(\"embed matrix at index:\\n\", embed_matrix[vocabulary[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def lime_predict(text):\n",
    "    \n",
    "    \n",
    "    final_output = []\n",
    "    predict_input = []\n",
    "    for text_variant in text:\n",
    "\n",
    "        \n",
    "        input_list_tokenized = [text_variant.split(' ')]\n",
    "        #input_list_tokenized = text_variant.split(' ')\n",
    "        print(input_list_tokenized)\n",
    "\n",
    "        \n",
    "        input_sequence = tokenizer.texts_to_sequences(input_list_tokenized)\n",
    "\n",
    "        \n",
    "        input_seq_padded = tensorflow.keras.preprocessing.sequence.pad_sequences(input_sequence,\n",
    "                                                                                 maxlen=15, \n",
    "                                                                                 padding='post', \n",
    "                                                                                 truncating='post')\n",
    "        #print('proper input:', input_seq_padded)\n",
    "\n",
    "        #result = model.predict(input_seq_padded)\n",
    "        #print(result[0].tolist())\n",
    "        #final_output.append(result[0].tolist())\n",
    "        \n",
    "        \n",
    "        predict_input.append(input_seq_padded[0].tolist())\n",
    "    \n",
    "    #print(predict_input)\n",
    "    \n",
    "    predict_output = lstm_tuned.predict(predict_input)\n",
    "    print(predict_output)\n",
    "    \n",
    "    #output_np = np.array(final_output) \n",
    "    #print(output_np)\n",
    "    \n",
    "    return predict_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.load_model('data/lstm2')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "pred_test_string = train['clean_string'][97]\n",
    "#print(pred_test_string)\n",
    "\n",
    "#print(lime_predict(pred_test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "explainer = lime_text.LimeTextExplainer()\n",
    "explanation = explainer.explain_instance(pred_test_string, lime_predict, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_model = False\n",
    "\n",
    "if not train_model:\n",
    "    lstm_tuned = models.load_model('data/lstm4')\n",
    "\n",
    "else:\n",
    "    lstm_tuned = models.Sequential()\n",
    "    lstm_tuned.add(layers.Input(shape=(15,)))\n",
    "    lstm_tuned.add(layers.Embedding(input_dim=embed_matrix.shape[0],\n",
    "                                    output_dim=embed_matrix.shape[1],\n",
    "                                    weights=[embed_matrix],\n",
    "                                    input_length=15,\n",
    "                                    trainable=False))\n",
    "    \n",
    "    lstm_tuned.add(layers.Bidirectional(layers.LSTM(units=15,\n",
    "                                                    dropout=0.2,\n",
    "                                                    return_sequences=True)))\n",
    "    \n",
    "    lstm_tuned.add(layers.Bidirectional(layers.LSTM(units=15,\n",
    "                                                    dropout=0.2)))\n",
    "    \n",
    "    lstm_tuned.add(layers.Dense(64, activation='relu'))\n",
    "    lstm_tuned.add(layers.Dense(32, activation='relu'))\n",
    "    \n",
    "    lstm_tuned.add(layers.Dense(2, activation='softmax'))\n",
    "    \n",
    "    lstm_tuned.compile(loss='sparse_categorical_crossentropy',\n",
    "                       optimizer='sgd',\n",
    "                       metrics=['accuracy'])\n",
    "    \n",
    "    lstm_tuned.summary()\n",
    "    \n",
    "    results_lstm_tuned = lstm_tuned.fit(train_seq_padded, y_train,\n",
    "                                        epochs=16,\n",
    "                                        batch_size=128,\n",
    "                                        validation_split=0.3,\n",
    "                                        verbose=1)\n",
    "    lstm_tuned.save('data/lstm6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_tuned.evaluate(test_seq_padded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_tuned.evaluate(test_seq_padded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nlp = dict()\n",
    "\n",
    "with open('data/glove.6B/glove.6B.300d.txt', 'r', encoding='utf8') as f:\n",
    "    w = 0\n",
    "    for line in f.readlines():\n",
    "        line = line.split(' ')\n",
    "        \n",
    "        try:\n",
    "            nlp[line[0]] = np.array(line[1:], dtype=float) \n",
    "            \n",
    "        except:\n",
    "            print('failed on line', w)\n",
    "            continue\n",
    "            \n",
    "print(nlp['candidate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix_glove = np.zeros((len(vocabulary)+1, 300))\n",
    "\n",
    "for word, index in vocabulary.items():\n",
    "    try:\n",
    "        embed_matrix[index] =  nlp[word]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_model = True\n",
    "\n",
    "if not train_model:\n",
    "    lstm_glove = models.load_model('data/lstm_glove1')\n",
    "\n",
    "else:\n",
    "    lstm_glove = models.Sequential()\n",
    "    lstm_glove.add(layers.Input(shape=(15,)))\n",
    "    lstm_glove.add(layers.Embedding(input_dim=embed_matrix.shape[0],\n",
    "                                    output_dim=embed_matrix.shape[1],\n",
    "                                    weights=[embed_matrix],\n",
    "                                    input_length=15,\n",
    "                                    trainable=False))\n",
    "    \n",
    "    lstm_glove.add(layers.Bidirectional(layers.LSTM(units=15,\n",
    "                                                    dropout=0.2,\n",
    "                                                    return_sequences=True)))\n",
    "    \n",
    "    lstm_glove.add(layers.Bidirectional(layers.LSTM(units=15,\n",
    "                                                    dropout=0.2)))\n",
    "    \n",
    "    lstm_glove.add(layers.Dense(64, activation='relu'))\n",
    "    lstm_glove.add(layers.Dense(32, activation='relu'))\n",
    "    \n",
    "    lstm_glove.add(layers.Dense(2, activation='softmax'))\n",
    "    \n",
    "    lstm_glove.compile(loss='sparse_categorical_crossentropy',\n",
    "                       optimizer='sgd',\n",
    "                       metrics=['accuracy'])\n",
    "    \n",
    "    lstm_glove.summary()\n",
    "    \n",
    "    results_lstm_glove = lstm_glove.fit(train_seq_padded, y_train,\n",
    "                                        epochs=16,\n",
    "                                        batch_size=128,\n",
    "                                        validation_split=0.15,\n",
    "                                        verbose=1)\n",
    "    lstm_glove.save('data/lstm_glove1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_glove.evaluate(test_seq_padded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "data_cleaned_all = pd.concat([train, test])\n",
    "\n",
    "print(len(data_cleaned_all))\n",
    "data_cleaned_all.reset_index(inplace=True)\n",
    "data_cleaned_all.drop(['Unnamed: 0', 'index'], axis=1, inplace=True)\n",
    "data_cleaned_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(data_cleaned_all.isna().sum())\n",
    "(data_cleaned_all.loc[data_cleaned_all['clean_string'].duplicated(keep=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data_cleaned_all.to_csv('data/cleaned_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tf_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_all = pd.read_csv('data/cleaned_all.csv')\n",
    "#something = tf_tokenizer(clean_all['clean_string'].to_list(), truncation=True)\n",
    "#print(type(something))\n",
    "clean_all.drop(['Unnamed: 0', 'body', 'lemma', 'tokenized', 'no_stop'], axis=1, inplace=True)\n",
    "clean_all['label'] = clean_all['REMOVED'].astype(int)\n",
    "clean_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "ds_test = Dataset.from_pandas(clean_all)\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(tf_input_ids))\n",
    "#print(tf_input_ids)\n",
    "#print(something[3])\n",
    "\n",
    "def tf_preprocess(input_dataset):\n",
    "    return tf_tokenizer(input_dataset['clean_string'], truncation=True)\n",
    "\n",
    "ds_tokenized = ds_test.map(tf_preprocess, batched=True)\n",
    "ds_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tf_tokenizer, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_or_smth = ds_tokenized.train_test_split(test_size=0.1, shuffle=True)\n",
    "\n",
    "dict_or_smth\n",
    "\n",
    "tf_train_set = dict_or_smth[\"train\"].to_tf_dataset(\n",
    "\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "\n",
    "    shuffle=True,\n",
    "\n",
    "    batch_size=16,\n",
    "\n",
    "    collate_fn=collator,\n",
    "\n",
    ")\n",
    "\n",
    "tf_validation_set = dict_or_smth[\"test\"].to_tf_dataset(\n",
    "\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "\n",
    "    shuffle=False,\n",
    "\n",
    "    batch_size=16,\n",
    "\n",
    "    collate_fn=collator,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in tf_train_set.take(1):\n",
    "  print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import create_optimizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "batches_per_epoch = len(dict_or_smth[\"train\"]) // batch_size\n",
    "\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#break into train,test,val splits\n",
    "#tf_X_train_val, tf_X_test, tf_y_train_val, tf_y_test = train_test_split(clean_all['clean_string'], clean_all['REMOVED'], \n",
    "#                                                                        random_state=8492, test_size=0.20)\n",
    "#tf_X_train, tf_X_val, tf_y_train, tf_y_val = train_test_split(tf_X_train_val, tf_y_train_val,\n",
    "#                                                             random_state=1228, test_size=0.125)\n",
    "\n",
    "#print(len(tf_X_test))\n",
    "#print(len(tf_X_val))\n",
    "#print(len(tf_X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#def tf_process(input_series):\n",
    "#    result = tf_tokenizer(input_series.to_list(), truncation=True)\n",
    "#    return result\n",
    "\n",
    "#tf_X_train_processed = tf_process(tf_X_train)\n",
    "\n",
    "#tf_X_val_processed = tf_process(tf_X_val)\n",
    "\n",
    "#tf_X_test_processed = tf_process(tf_X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
